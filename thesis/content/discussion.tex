%Interpretation of the research results
%Comparison of the results with those of the previous studies
%Contributions of your research to the existing knowledge body
%Limitations of the study
%Unexpected results
%How the results support or refute a certain theory or findings of the previous studies

%Opinionated discussion about the results
%Provide argumentation for 
\chapter{Discussion}\label{discussion}
After presenting the results in the last chapter, this chapter will interpret the results to propose the best approach for solving our problem. 

\section{Performance results}\label{discussion:performance}
This section discusses the performance criteria results presented in the last chapter. The measurement process we used for the performance results produces relevant results for our problem. The reason is that we benchmark with a ``warmup''-phase on a virtual machine architecture (see \ref{methods-for-data-collection}) to simulate the conditions of a running cloud-based IDE. To compare the performance of different approaches, we tried to measure the execution beginning from identical start-points and ending at the same result (outlined in \ref{results:performance}) and with about the same packages installed (see \ref{appendix:bench-envs-packages}) to get comparable results. 

We can summarize from the BIAR approach-specific benchmarks results that saving I/O operations and disk/cache space tends to pay off in the first-time build performance for the streamed build strategy. According to the Nix documentation, we assumed that the streamed strategy saves I/O operations and disk/cache space. We could make a stronger argument if we verified this assumption specifically with benchmarks. As expected, the subsequent-time build performance did not result in statistically significant differences between both strategies since Nix uses the cached layers and does not rebuild for both strategies. Therefore we should opt for the streamed strategy implementation.

For the NSAR approach, we can conclude that the results matched our expectations. This implies that we should use a cached nix-shell, and we could use a seeded Nix store from a performance standpoint to improve the first-time build. The cached nix-shell is slower for the first-time build performance than the uncached nix-shell, but this is not problematic since we can be slower in this case. The cached nix-shell allows us to achieve an up to three times faster subsequent-time startup compared to not using caching. This result is somewhat similar to the very limited data (of an unknown measurement process) of \cite{CachedNixShell} where a slighter difference was measured between the cached and uncached nix-shell. 

The results of the seeded store evaluation also indicate that adding a few packages, which are available in the binary cache, to a previously built configuration results in a small overhead compared to the first-time build. This result infers that Nix can efficiently cache previous builds in the Nix store and also transfers to the BIAR prototype since Nix efficiently caches both the nix-shell execution and the resulting layers of an image build as derivations in the Nix store (see \ref{hermeticity}, \ref{efficient-caching-of-layers}).

Recall that in \ref{results-seeded-nix-store}, we measured a more negligible overhead when building six additional packages in the Python environment that were not in the seeded store than building one package in the \verb|C++| configuration change. This result is unexpected since we predicted the opposite. The reason for this result could be that the dependencies of the additional six packages have a large intersection with the dependencies of packages already in the seeded store. This would confirm our expectation outlined in \ref{BIAR-execution-flow} and aligns with the argumentation of \cite{Christensen2018}. However, we can not make a decisive argument since the packages added to each environment are different and, obviously, the environments themselves (see \ref{test-environments}). 

To summarize, even though the seeded Nix store improves the first-time build, it does not provide a strong enough argument to justify its implementation for our problem. This is because, after a first-time build, the resulting Nix store is comparable to a seeded store from the viewpoint of subsequent builds with small configuration changes. Our argument makes sense for our problem since suppose a running cloud-based IDE in which we provide configuration templates that most lecturers will not change or only slightly. Then after the first build, which produces a ``seeded store'', all other lecturers' configuration changes will only experience a small overhead.

This discussion on the NSAR implementation has implications: caching the nix-shell is required to achieve fast environment startups for subsequent-time invocations, and implementing a seeded Nix store is unnecessary. The former implies that we must implement the unsafe shared nix-shell evaluation cache (see \ref{cross-cache-attacks-on-nix-shell-cache}). %We will use the idea of a seeded store in the further work discussion.

The first-time build performance of the NSAR prototype is the fastest among all three approaches (see \ref{first-time-build}) and even can be optimized with a seeded store, as argued before. The prototype of the BIAR approach has a comparable first-time build performance to the current approach, as argued in the \verb|C++| environment results of \ref{prototypes-vs-current-first-time-build}. Unlike the current approach, the BIAR and the NSAR prototypes automatically optimize the build time for small configuration changes, but in the case of BIAR, this optimization is currently limited by the push-time as discussed in \ref{biar-approach-specific}.

The results of the subsequent-time build performance were as expected. The overhead measured for checking the nix-shell cache (300ms) in the NSAR prototype align with the, again, very restricted result (290ms) from \cite{CachedNixShell}. This overhead is roughly three times the time it takes to compute the hash value over the configuration in the BIAR prototype. This hash value computation might also be the difference in the subsequent startup performance between the BIAR prototype and the current approach since they both start a container without a network and without checking cached results like the NSAR prototype. This means that it is essential for our problem to optimize the cache verification in the NSAR prototype and the hash value computation in the BIAR implementation. The cache verification is challenging to optimize as it is already optimized and an external package outside our prototype implementation. 

%TODO: scaling
In our benchmarks, we used two test environments: a \verb|C++| -- to represent compiled languages -- and a Python one -- to represent interpreted languages. We wanted to evaluate whether one approach is more suited for one environment than the other or if they perform equally well regardless of the environment. Our results from \ref{first-time-build} and \ref{subsequent-time-build} imply that no approach is more suited to one type of environment over the other. 

\section{Security results}\label{discussion:security}
The shared cache attack vector results showed that the NSAR prototype has more security vulnerabilities than the BIAR prototype for two main reasons. The first, apart from the shared Nix store, is the additional cache for part of the nix-shell execution. This cache has no barriers against cross-cache attacks, resulting in security vulnerabilities or performance degradation for other users. The second is due to a reasonable assumption in our context that the lecturers have no intention to maliciously attack the system since they configure the environments for teaching purposes. This assumption holds for the network attack vector as well. 

Another point is that the shared cache security vulnerability could easily be removed from the BIAR approach by just removing the shared Nix store from the builder container without impacting the subsequent-time build performance. The implication would be that the build performance for configuration changes would equal the first-time build with empty build caches. This quick-fix would tradeoff performance for security. This stands in contrast to the NSAR prototype, where the shared Nix store and shared nix-shell cache is required to achieve the essential quick subsequent time environment startup. Therefore the BIAR prototype has even more advantages from a security standpoint than the NSAR prototype.

As argued in \ref{cross-cache-attacks-on-nix-store}, installing Nix in multi-user mode would improve the security of the shared Nix store to a small extent. This would, however, come at the cost of needing to run the Nix daemon inside each environment and configuring custom \verb|systemd| initialization mechanisms \cite{NixMultiUser}. The former could result in performance degradation and the latter in poor developer experience. Therefore we conclude that the implementation tradeoffs prevail over the security gains for implementing the multi-user mode.

To summarize, the security vulnerabilities of both prototypes have their cause in which part of the prototype's architecture the Nix build system is installed. Since the resulting environments of the BIAR prototype and obviously of the current approach do not have Nix installed, the environments do not have a shared cache nor network access and therefore have none of the two attack vectors. This contrasts with the NSAR approach, where the environment needs to have Nix installed.
% network access, -> exam situation
\section{User and developer experience results}\label{discussion:UX-and-DevX}
The results are based on our analysis of the prototypes and current approaches implementations based on the enumerated properties of what we define as a ``good'' user or developer experience. For the developer and user experience results, we do not have results from previous studies for comparison. This is because of our particular use cases in the context of a cloud-based IDE where we have developers on one side and lecturers and students on the other. Each side has individual requirements on the system and challenges with the current approach. 

\subsection{User experience}\label{discussion:UX}
From the results of \ref{user-experience-config}, it seems that our prototypes have a slight advantage. However, we cannot decisively conclude if they provide an improved UX over the current approach regarding the ease of writing configurations. Indeed they have the edge for composing configurations and for simple configuration changes, considering the challenge of writing a custom \verb|Dockerfile| for the current approach. However, whether the advantages outweigh the fact that the lecturer needs to take on more responsibility with our prototypes is unclear. Furthermore, some details -- e.g., figuring out the correct environment variables -- of configuring custom configurations with Nix could be more or less challenging. To make a more decisive argument, testing the configuration process of our prototypes with lecturer feedback would be beneficial.

Regarding boosting the flexibility of configuration, our prototypes clearly have the edge over the current approach. Users can efficiently change the configuration with our prototypes and test the effects on the production systems within a few minutes instead of possibly multiple days for the current approach. Since the subsequent startup time of the current approach is slightly faster than BIAR and much faster than NSAR, the UX for executing code in environments tends to be better for the current approach. To make a more decisive argument in this case, we would need to test our prototypes with student feedback to find out whether, e.g., the small startup overhead of the BIAR prototype compared to the current approach impacts the user experience poorly. 

\subsection{Developer experience}\label{discussion:DevX}
The developer's workload from configuration changes by the lecturer is undoubtedly minimized for our prototypes as opposed to the current approach, where configuration changes are likely to be very demanding. Our prototypes also have the advantage over the current approach as the developer has less responsibility for usage defects if the lecturer does not set up the environment correctly. Thus our approaches have a much better developer experience for both points. 

The further development and testing of our prototypes can be challenging initially. However, it is hard to make a powerful argument on how much the tools provided by Nix for development and testing alleviate the ease of implementing and developing our prototypes compared to the current approach in the long run. Feedback from developers who worked with the prototypes would help make a more solid argument for this aspect.

Our prototypes have one or two base images and few dependencies to maintain, resulting in a better developer experience than the current approach. The advantage of our prototypes might shrink from maintaining the Nixpkgs and Nix versions, the shared Nix store, the nix-shell cache (for NSAR), and the image registry (for BIAR). Maintaining the nix-shell cache and additional dependency (\verb|cached-nix-shell|) of the NSAR prototype likely overweighs the burden of the BIAR prototype's additional image and image registry. Our prototypes offer more robustness and are future-proof as they are reproducible.
%To summarize, the maintainability for the developer is better for our prototypes compared to the current approach.